clickhouse   https://blog.csdn.net/china_world/article/details/52687685?locationNum=1
             https://blog.csdn.net/u013676711/article/details/78862243

Hortonworks的hadoop发行版HDP中,数据治理包括Falcon和Atlas这两个组件
   Atlas主要负责元数据的管理. Falcon主要负责数据生命周期的管理
总结来说, Falcon是在调度器Oozie上封装了一层,用户可以用Web UI上的wizard来配置数据流水线, 数据生命周期管理非常方便. 它最大的优点就是增强了Oozie的易用性, 对于业务逻辑比较复杂的系统, 用Falcon管理比起直接用Oozie会大大简化.
但是调研中发现, Falcon Web UI上呈现的血缘关系只是以实体为中心, 并没有全局层面上整个data pipeline的血缘关系. 如果能够以pipeline为中心, 画出血缘关系图,提供zoom in功能, 以及在图中把实体运行状态可视化, 将会是一个很有用的特性.
虽然被称为数据治理工具,但是它的功能只是集中在数据生命周期管理和流水线管理,要与元数据管理(Atlas),数据安全管理(Ranger)等组合,才能形成一个完整的数据治理解决方案.
调研发现, Falcon的关注度并不算高,能搜到的英文资料主要是Apache和Hortonworks官网,中文资料几乎没有. 目前也没有看到哪个大公司在生产环境中用Falcon,我认为主要是因为Falcon提供的功能单一, 只解决了数据治理领域的一小部分需求,用户更愿意使用集中化的数据治理工具,或者自己开发.

Apache Atlas为Hadoop集群提供了包括数据分类、集中策略引擎、数据血缘、安全和生命周期管理在内的元数据治理核心能力，其与Apache Falcon，Apache Ranger相互整合可以形成完整的数据治理解决方案。但是Atlas目前还是Apache孵化项目，尚未成熟，有待发展



大数据学习qq群:894951460

spark grouping sets 等操作 https://blog.csdn.net/u011622631/article/details/84786777
spark sql优化案例 https://mp.weixin.qq.com/s?__biz=MzU5NTc1NzE2OA==&mid=2247483913&idx=1&sn=6d7736eb43dfbbdf25508d266d8606f7&chksm=fe6c5316c91bda00a8d2d34eb6ea92e2eb85a21f45586a0b7f71ff92745131d51ea247d4f112&scene=21#wechat_redirect

技术博客 http://blog.didispace.com/

查看分区下数据块健康信息和数据块大小
hdfs fsck hdfs://ns:8020/hive/db/db.db/table/dt=2020-04-07 -files -blocks
hive --orcfiledump  path

set hive.exec.orc.split.strategy=ETL;
set mapreduce.input.fileinputformat.split.maxsize = 256000000

set mapred.reduce.tasks=10;
set hive.exec.orc.split.strategy=ETL;
set mapreduce.input.fileinputformat.split.maxsize = 26000000;
insert overwrite table tb(dt = 'test_04_14')
select  data
 from tb
where dt = '2020-04-14' distribute by rand(123);


java -server -Xms3036m -Xmx8240m -verbose:gc -XX:+UseG1GC -XX:PermSize=128m -XX:MaxPermSize=128m -Xss256k -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpP
ath=/export/rtdw/logs/$1.log  -XX:+PrintGCDetails -jar /export/rtdw/translate-11.8.nocheck-SNAPSHOT.jar --spring.profiles.active=pro --dbrep.node.name=
$1 --dbrep.version=$2 --reader.distribute.num=${distributeNum} --log.path=/export/rtdw/logs/$1 >> /export/rtdw/logs/$1/translate.log 2>&1 &
jps -ml|grep TaskRunnerApplicationBootstrap|awk '{print $1}'| xargs -n 1 -I [] awk '/^Swap:/ {SWAP+=$2}END{print SWAP" KB  []"}' /proc/[]/smaps

spark steaming exactly once:
    structured streaming:通过checkpoint wal和可重放的源数据来保证数据不丢失,sink需保证幂等性

flink 使用jobmanager通过checkpointcoordinator向source中发送checkpointbarrier，栅栏流到一个算子，该算子暂停运算，并将自身状态存入statebackend,通过栅栏对齐实现内部exactly once,通过不对齐实现at least once
我程序中Flink的CheckPoint语义设置了 Exactly Once，但是我的mysql中看到数据重复了？程序中设置了1分钟1次CheckPoint，但是5秒向mysql写一次数据，并commit；
      答：Flink要求end to end的精确一次都必须实现TwoPhaseCommitSinkFunction。如果你的chk-100成功了，过了30秒，由于5秒commit一次，所以实际上已经写入了6批数据进入mysql，但是突然程序挂了，从chk100处恢复，这样的话，之前提交的6批数据就会重复写入，所以出现了重复消费。Flink的精确一次有两种情况，一个是Flink内部的精确一次，一个是端对端的精确一次，这个博客所描述的都是关于Flink内部去的精确一次，我后期再发一个博客详细介绍一下Flink端对端的精确一次如何实现

hive --hiveconf hive.root.logger=DEBUG,console

JSqlParser 0.9.7 发布，SQL 解析工具
第一步：通过MR生成hbase对应HFile文件
第二步：hadoop distcp跨集群拷贝
第三步：bulkload

bulkload注意事项：
    mapreduce方式生成hfile则reducer数量和hbase表预分区数据量一样
    bulkload每个region默认限制Hfile数量小于32个
        hbase.hregion.max.filesize
	hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily
   bulkload数据量限制为上述两个参数的乘积
http://hbase.apache.org/book.html#arch.bulk.load
http://www.cloudera.com/documentation/enterprise/5-3-x/topics/admin_hbase_import.html

ctrl alt b   查看实现
f12          打开命令行
alt ;        显示当前类结构
ctrl q       显示注释文档
Ctrl + shift + alt + n 查找类中方法或变量
Ctrl+Shift+U 大小写转化

【调试部分、编译】
Ctrl+F2，停止
Alt+Shift+F9，选择 Debug
Alt+Shift+F10，选择 Run
Ctrl+Shift+F9，编译
Ctrl+Shift+F10，运行
Ctrl+Shift+F8，查看断点
F8，步过
F7，步入
Shift+F7，智能步入
Shift+F8，步出
Alt+Shift+F8，强制步过
Alt+Shift+F7，强制步入
Alt+F9，运行至光标处
Ctrl+Alt+F9，强制运行至光标处
F9，恢复程序
Alt+F10，定位到断点
Ctrl+F8，切换行断点

ctrl h
ctrl alt h


clouderal pom配置
pom添加：
<repositories>
	<repository>
		<id>cloudera</id>
		<url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
		<releases>
			<enabled>true</enabled>
		</releases>
		<snapshots>
			<enabled>true</enabled>
		</snapshots>
	</repository>
</repositories>
修改maven settings.xml
*,!cloudera表示出了aliyun还可以使用cloudera
<mirror>
	<id>nexus-aliyun</id>
	<mirrorOf>*,!cloudera</mirrorOf>
	<name>Nexus aliyun</name>                     
	<url>
	  http://maven.aliyun.com/nexus/content/groups/public
	</url>
</mirror> 
















