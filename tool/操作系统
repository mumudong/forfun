批量修改文件名 2g是替换第二个{}对应的名字
find node -type f -name '*\.jsjsjsjs' | xargs -i echo mv \"{}\" \"{}\" | sed 's/jsjsjsjs/js/2g' | sh
xargs -I gg mv gg gg_test
gg是每个元素的别名

dig指令也是一个做 DNS 查询的。不过dig指令显示的内容更详细
host www.baidu.com
dig www.baidu.com

  网卡接收到数据,会由dma直接映射到内存,网卡内存很小,只做简单缓存
  linux中用一个双向链表做缓冲区,buffer看上去很像有很多凹槽的线性结构,每个凹槽存储一个封包(从网络层看是ip封包,从传输层看是tcp封包)
  操作系统从buffer取数据,经过协议栈处理形成socket文件
  如果高并发把buffer占满,操作系统会拒绝服务(防止并发太大雪崩),拒绝服务攻击
  传入网卡的数据frames,frame是数据链路层的传输单位,网卡使用dma将frame写入buffer，然后出发CPU中断交给操作系统处理,
    操作系统取出frame经由协议栈进行还原形成socket结构(根据ip和端口区分不同socket文件),进程读取socket获取数据
  Unix系统中socket是一种特殊文件,双向管道,一端是应用,一端是缓冲区
  服务端的socket文件会存在操作系统内核中,绑定到某个ip和端口，accept的时候服务端会拿到客户端的socket描述符会写到服务端的socket文件中
  io多路复用：上面进程拿到了关注的所有socket集合,如何监听关注结合的状态变化？
     一个线程需要处理所有关注socket的变化,要处理很多文件io,构成了io的多路复用问题
     需要操作系统提供内核级别的支持,linux有三种多路复用api

     内核知道哪些socket有变化
      select: select(read_set,write_set,err_set),然后遍历判断是某个事件并做对应处理，内核一次处理1024个描述符
      poll:select不是好编程模型,好的应该是网络变化时会发生事件,用户不需要select设置自己的集合,而是通过系统api直接拿到对应的消息,处理对应的文件描述符
      epoll:从性能角度分析poll和select差距不大,因为内核产生一个消息后仍要遍历poll关注的所有描述符来判断消息是否和用户进程有关
         为了解决上述问题，epoll将进程关注的文件描述符存入二叉搜索树,通常是红黑树,key是socket编号,值是关注的消息,当内核发生了一个事件,如socket1000可读
         此时从红黑树找到进程是否关注这个事件,有关注的事情发生时epoll会先放到一个队列中,用户调用时从队列返回
         select/poll是阻塞,epoll是非阻塞,多数情况下epoll性能好是因为红黑树的实现


    磁盘：内存支持字节级别随机存取,而在磁盘中通常不支持
      机械磁盘：读取数据要转动物理磁头,时间开销大,一次读取一两个字节的数据非常不划算
      固态硬盘：可随机存取，单硬盘速度远不及内存,连续读多个字节的速度远不如一次读一个硬盘块
      为了提高性能,会将物理存储划分成一个个小块,如4kb，这样也让硬盘的使用看起来整齐，便于回收空间，且知道块的序号就可算出物理位置
    数据从磁盘要内存要经过dma、总线等耗时，一次操作一个块才是可行方案
      硬盘分块后如果用硬盘存储文件就是文件系统的事情
          fat格式：占内存，（file allocate table）需要在内存中维护块和文件的对应关系
          iNode传统文件

    行存：数据一个接一个排列,一行数据跟着另一行,更新一行的操作常可在一个块中进行,而查询聚合往往需要跨块
       特点：更新快，单条记录数据集中，适合事物(列式存储要更新多个位置数据，不适合事物)
            查询慢
    索引：用来进行查询的额外数据
    B树：所有节点存储数据
    B+数：叶子节点存储数据，内存可加载更多树层级，叶子间有序连接便于范围查询

    tcp/udp:
       tcp功能大而全、udp灵活高效
       可靠性：
          tcp支持可靠性，udp不支持
          可靠性手段：
            校验和：接收到数据包字节数组后对字节进行异或操作得到一个值，与数据包特定的校验和比对，不同则数据有损坏
                tcp和udp都实现了校验和,tcp发现校对不上会丢弃封包并且重发,udp什么都不会处理，把处理权交给使用它的程序员
            请求、应答、连接：tcp实现了，udp没有实现
                如果发送方在一段时间内没有收到应答,发送方会重新发送
                三次握手：
                  alice和bob想建立连接,alice给Bob发送一个建立连接的消息，在tcp协议中称为同步sync
                  Bob收到sync给Alice一个响应，在tcp协议中称为响应ack
                  Alice给Bob sync，Bob给Alice ack，此时对Alice来说连接建立成功了
                  但是tcp是双工协议，数据可以双向传输，对Bob来说连接还没建立，因为Bob马上给Alice发消息，消息可能先于Bob发的ack到达，此时Alice不知道连接建立成功
                  所以解决办法是Bob给Alice发sync  Alice在给Bob一个ack，就是三次握手
                四次挥手：
                  中断连接的请求称为finish（用fin标识）
                  1、Alice发送fin
                  2、Bob回复ack
                  3、Bob发送fin（可能存在发给Alice但没收到ack的消息，因此需要在自己准备妥当后再发送）
                  4、Alice回复ack


       连接是很好的变成模型，连接不稳时可断开重连，增加了应用间的数据传输可靠性
       封包排序：可靠性要求数据有序发出，无序传输，有序组合，tcp保证这种可靠性
          传输之前数据分块tcp segment，udp datagram，到达目的地是乱序，要重新排序回复顺序
          tcp利用滑动窗口、快速重传等算法保证顺序,udp仅仅给datagram标注序号，没有帮助程序进行数的排序

      进程间通信：
          单机：
            管道：管道的核心是不侵入、灵活，不会增加程序设计负担，又能组织复杂的计算过程
            共享内存：速度快效率高，但是编程要求高
                  Linux 内存共享库的实现原理是以虚拟文件系统的形式，从内存中划分出一块区域，供两个进程共同使用。看上去是文件，实际操作是内存
                  共享内存的方式，速度很快，但是程序不是很好写，因为这是一种侵入式的开发，也就是说你需要为此撰写大量的程序。
                  比如如果修改共享内存中的值，需要调用 API。如果考虑并发控制，还要处理同步问题等。
                  因此，只要不是高性能场景，进程间通信通常不考虑共享内存的方式。

            本地消息/队列
                  内存共享不太好用，因此本地消息有两种常见的方法。
                  一种是用消息队列——现代操作系统都会提供类似的能力。 Unix 系可以使用 POSIX 标准的 mqueue。
                  另一种方式，就是直接用网络请求，比如 TCP/IP 协议，也包括建立在这之上的更多的通信协议

            远程调用
                  远程调用（Remote Procedure Call，RPC）是一种通过本地程序调用来封装远程服务请求的方法。
                  程序员调用 RPC 的时候，程序看上去是在调用一个本地的方法，或者执行一个本地的任务，但是后面会有一个服务程序（通常称为 stub），
                  将这种本地调用转换成远程网络请求。 同理，服务端接到请求后，也会有一个服务端程序（stub），将请求转换为一个真实的服务端方法调用。

                      RPC 调用的方式比较适合微服务环境的开发，当然 RPC 通常需要专业团队的框架以支持高并发、低延迟的场景。
                  不过，硬要说 RPC 有额外转化数据的开销（主要是序列化），也没错，但这不是 RPC 的主要缺点。
                     RPC 真正的缺陷是增加了系统间的耦合。当系统主动调用另一个系统的方法时，就意味着在增加两个系统的耦合。
                     长期增加 RPC 调用，会让系统的边界逐渐腐化。这才是使用 RPC 时真正需要注意的东西。

            消息队列
                  既然 RPC 会增加耦合，那么怎么办呢——可以考虑事件。
                  消息队列是一种耦合度更低，更加灵活的模型。但是对系统设计者的要求也会更高，对系统本身的架构也会有一定的要求
                  具体场景的消息队列有 Kafka，主打处理 feed；
                  RabbitMQ、ActiveMQ、 RocketMQ 等主打分布式应用间通信（应用解耦）。


      线程调度：
         先到先服务FCFS ：
             这里需要用到一个叫作队列的数据结构，具有先入先出（First In First Out，FIFO）性质。
             先进入队列的作业，先处理，因此从公平性来说，这个算法非常朴素。
             另外，一个作业完全完成才会进入下一个作业，作业之间不会发生切换，从吞吐量上说，是最优的——因为没有额外开销。
         短作业优先SJF ：
             通常会同时考虑到来顺序和作业预估时间的长短，我们还可以从另外一个角度来审视短作业优先的优势，就是平均等待时间。
             在大多数情况下，应该优先处理用时少的，从而降低平均等待时长。

         采用 FCFS 和 SJF 后，还有一些问题没有解决。
             紧急任务如何插队？比如老板安排的任务。
             等待太久的任务如何插队？比如用户等太久可能会投诉。
             先执行的大任务导致后面来的小任务没有执行如何处理？比如先处理了一个 1 天才能完成的任务，工作半天后才发现预估时间 1 分钟的任务也到来了。

         为了解决上面的问题，我们设计了两种方案，
             一种是优先级队列（PriorityQueue）
             另一种是抢占（Preemption）。

             比如老板的任务，就给一个更高的优先级。 而对于普通任务，可以在等待时间（W） 和预估执行时间（P） 中，找一个数学关系来描述。比如：优先级 = W/P。W 越大，或者 P 越小，就越排在前面。 当然还可以有很多其他的数学方法，利用对数计算，或者某种特别的分段函数。

         多级队列模型
           多级队列，就是多个队列执行调度
              队列1 紧急任务....
              队列2 普通任务....

           上图中设计了两个优先级不同的队列，从下到上优先级上升，上层队列调度紧急任务，下层队列调度普通任务。只要上层队列有任务，下层队列就会让出执行权限。
            低优先级队列可以考虑抢占 + 优先级队列的方式实现，这样每次执行一个时间片段就可以判断一下高优先级的队列中是否有任务。
            高优先级队列可以考虑用非抢占（每个任务执行完才执行下一个）+ 优先级队列实现，这样紧急任务优先级有个区分。如果遇到十万火急的情况，就可以优先处理这个任务。
           上面这个模型虽然解决了任务间的优先级问题，但是还是没有解决短任务先行的问题。可以考虑再增加一些队列，让级别更多。比如下图这个模型：

              队列1 紧急任务....
              队列2 小时间片....
                    普通任务先放到优先级仅次于高优任务的队列中，并且只分配很小的时间片；
                    如果没有执行完成，说明任务不是很短，就将任务下调一层
              队列3 大时间片....

